## Table of Contents
<table>
<tr><td colspan="2"><a href="#1-data" style="color:#B22222">1. Data</a></td></tr>

<tr><td colspan="2"><a href="#2-visual-encoder" style="color:#B22222">2. Visual Encoder</a></td></tr>

<tr><td colspan="2"><a href="#3-modality-aligning" style="color:#B22222">3. Modality Aligning</a></td></tr>

<tr><td colspan="2"><a href="#4-from-llm" style="color:#B22222">4. From LLM</a></td></tr>
</table>

## [1. Data](#content)
1. [Ciem: Contrastive instruction evaluation method for better instruction tuning](), NeurIPS workshop 2023, \
Hongyu Hu, Jiyuan Zhang, Minyi Zhao, et al.
1. [Mitigating hallucination in large multi-modal models via robust instruction tuning](), arXiv 2023, \
Fuxiao Liu, Kevin Lin, Linjie Li, et al.
1. [Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback](), arXiv 2023, \
Tianyu Yu, Yuan Yao, Haoye Zhang, et al.
1. [Mitigating hallucination in large multi-modal models via robust instruction tuning](), arXiv 2023, \
Fuxiao Liu, Kevin Lin, Linjie Li, et al.

## [2. Visual Encoder](#content)
1. [Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption](), arXiv 2023, \
Bohan Zhai, Shijia Yang, Xiangchen Zhao
1. [Monkey: Image resolution and text label are important things for large multi-modal models](), arXiv 2023, \
Zhang Li, Biao Yang, Qiang Liu, et al.
1. [Vcoder: Versatile vision encoders for multimodal large language models](), arXiv 2023, \
Jitesh Jain, Jianwei Yang, and Humphrey Shi.
1. [Fine-grained image captioning with clip reward](), NAACL Findings 2022, \
Jaemin Cho, Seunghyun Yoon, Ajinkya Kale, et al.

## [3. Modality Aligning](#content)
1. [Aligning large multimodal models with factually augmented rlhf](), arXiv 2023, \
Zhiqing Sun, Sheng Shen, Shengcao Cao, et al.
1. [Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization](), arXiv 2023, \
Zhiyuan Zhao, Bin Wang, Linke Ouyang, et al.
1. [Hallucination augmented contrastive learning for multimodal large language model](), arXiv 2023, \
Chaoya Jiang, Haiyang Xu, Mengfan Dong, et al.
1. [Woodpecker: Hallucination correction for multimodal large language models](), arXiv 2023, \
Shukang Yin, Chaoyou Fu, Sirui Zhao, et al.
1. [Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks](), arXiv 2023, \
Zhe Chen, Jiannan Wu, Wenhai Wang, et al.

## [4. From LLM](#content)
1. [A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions](https://dl.acm.org/doi/full/10.1145/3703155), ACM TIS 2025, \
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, Ting Liu
1. [Vigc: Visual instruction generation and correction](), arXiv 2023, \
Bin Wang, Fan Wu, Xiao Han, et al.
1. [Neural path hunter: Reducing hallucination in dialogue systems via path grounding](), EMNLP 2021, \
Nouha Dziri, Andrea Madotto, Osmar R Zaiane, et al.
1. [The curious case of neural text degeneration](), ICLR 2020, \
Ari Holtzman, Jan Buys, Li Du, et al.