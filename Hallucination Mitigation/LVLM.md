## Introduction

Hallucination mitigation in Large Vision-Language Models (LVLMs) focuses on reducing instances where the model generates outputs that are factually incorrect, logically inconsistent, or not grounded in the given input. These hallucinations often arise from overgeneralization, spurious correlations, or limitations in aligning visual and textual representations. Effective mitigation strategies combine improved model architectures, training with high-quality multimodal datasets, alignment techniques such as reinforcement learning with human feedback, and post-hoc methods like retrieval augmentation or uncertainty estimation. Together, these approaches aim to enhance the reliability, trustworthiness, and safety of LVLMs in real-world applications.

## Table of Contents

<table>
  <tr><td colspan="2"><a href="#1-methods-and-models" style="color:#B22222">1. Methods and Models</a></td></tr>
  <tr>
    <td>&ensp;<a href="#11-inference-approaches">1.1 Inference Approaches</a></td>
    <td>&ensp;<a href="#12-datasets-and-benchmarks">1.2 Datasets and Benchmarks</a></td>
  </tr>

</table>


## [1. Methods and Models](#content)
### [1.1 Inference Approaches](#content)
1. [The Hidden Life of Tokens: Reducing Hallucination of Large Vision-Language Models Via Visual Information Steering](https://arxiv.org/abs/2502.03628), ICML 2025, \
Zhuowei Li, Haizhou Shi, Yunhe Gao, Di Liu, Zhenting Wang, Yuxiao Chen, Ting Liu, Long Zhao, Hao Wang, Dimitris N. Metaxas
1. [SECOND: Mitigating Perceptual Hallucination in Vision-Language Models via Selective and Contrastive Decoding](https://arxiv.org/abs/2506.08391), ICML 2025, \
Woohyeon Park, Woojin Kim, Jaeik Kim, Jaeyoung Do
1. [Mitigating Object Hallucination in Large Vision-Language Models via Image-Grounded Guidance](https://arxiv.org/abs/2402.08680), ICML 2025, \
Linxi Zhao, Yihe Deng, Weitong Zhang, Quanquan Gu
1. [Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models](https://arxiv.org/abs/2410.03577), ICML 2025, \
Xin Zou, Yizhou Wang, Yibo Yan, Yuanhuiyi Lyu, Kening Zheng, Sirui Huang, Junkai Chen, Peijie Jiang, Jia Liu, Chang Tang, Xuming Hu
1. [HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding](https://arxiv.org/abs/2403.00425), ICML 2024, \
Zhaorun Chen, Zhuokai Zhao, Hongyin Luo, Huaxiu Yao, Bo Li, Jiawei Zhou


### [1.2 Datasets and Benchmarks](#content)
1. [Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2406.16449), ICML 2024, \
Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, Rongrong Ji
