## Table of Contents
<table>
<tr><td colspan="2"><a href="#1-detection" style="color:#B22222">1. Detection</a></td></tr>
<tr>
  <td>&ensp;<a href="#11-automated-post-hoc-detection-models">1.1 Automated Post hoc Detection Models</a></td>
  <td>&ensp;<a href="#12-human-in-the-loop-detection">1.2 Human in the loop Detection</a></td>
</tr>
<tr>
  <td>&ensp;<a href="#13-task-specific-detection">1.3 Task Specific Detectors</a></td>
  <!-- <td>&ensp;<a href="#12-human-in-the-loop-detection">1.2 Human in the loop Detection</a></td> -->
</tr>

<tr><td colspan="2"><a href="#2-evaluation-methods" style="color:#B22222">2. Evaluation Methods</a></td></tr>
<tr><td colspan="2"><a href="#3-benchmarks" style="color:#B22222">3. Benchmarks</a></td></tr>

</table>

## [1. Detection](#content)
1. [HalLoc: Token-level Localization of Hallucinations for Vision Language Models](https://arxiv.org/abs/2506.10286), CVPR 2025, \
Eunkyu Park, Minyeong Kim, Gunhee Kim
1. [Hallo3D: Multi-Modal Hallucination Detection and Mitigation for Consistent 3D Content Generation](https://proceedings.neurips.cc/paper_files/paper/2024/hash/d75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html), NeurIPS 2024, \
Hongbo Wang, Jie Cao, Jin Liu, Xiaoqiang Zhou, Huaibo Huang, Ran He
1. [Estimating the Hallucination Rate of Generative AI](https://nips.cc/virtual/2024/poster/95553), NeurIPS 2024, \
Andrew Jesson, Nicolas Beltran Velez, Quentin Chu, Sweta Karlekar, Jannik Kossen, Yarin Gal, John Cunningham, David Blei

### [1.1 Automated Post hoc Detection Models](#content)
1. [Visual hallucination detection in large vision-language models via evidential conflict](https://www.sciencedirect.com/science/article/abs/pii/S0888613X25001483), ICAR 2025, \
Tao Huang, Zhekun Liu, Rui Wang, Yang Zhang, Liping Jing

### [1.2 Human in the loop Detection](#content)
1. [LLaVA-RLHF: Aligning Large Multimodal Models with Factually Augmented RLHF](https://llava-rlhf.github.io/), arXiv 2023, \
Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, Trevor Darrell

### [1.3 Task Specific Detectors](#content)


## [2. Evaluation Methods](#content)
1. [Evaluation and mitigation of agnosia in multimodal large language models](), arXiv 2023, \
Jiaying Lu, Jinmeng Rao, Kezhen Chen, et al.
1. [Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption](), arXiv 2023, \
Bohan Zhai, Shijia Yang, Xiangchen Zhao, et al.
1. [Faithscore: Evaluating hallucinations in large visionlanguage models](), arXiv 2023, \
Liqiang Jing, Ruosen Li, Yunmo Chen, et al.
1. [Mitigating hallucination in large multi-modal models via robust instruction tuning](), arXiv 2023, \
Fuxiao Liu, Kevin Lin, Linjie Li, et al.
1. [Aligning large multimodal models with factually augmented rlhf](), arXiv 2023, \
Zhiqing Sun, Sheng Shen, Shengcao Cao, et al.
1. [Detecting and preventing hallucinations in large vision language models](), arXiv 2023, \
Anisha Gunjal, Jihan Yin, and Erhan Bas
1. [Instructblip: Towards general-purpose visionlanguage models with instruction tuning](), arXiv 2023, \
Wenliang Dai, Junnan Li, Dongxu Li, et al.
1. [Evaluation and analysis of hallucination in large vision-language models](), arXiv 2023, \
Junyang Wang, Yiyang Zhou, Guohai Xu, et al.

## [3. Benchmark](#content)
1. [ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2409.09318), CVPR 2025, \
Yahan Tu, Rui Hu, Jitao Sang
1. [PhD: A ChatGPT-Prompted Visual Hallucination Evaluation Dataset](https://huggingface.co/datasets/AIMClab-RUC/PhD), CVPR 2025, \
Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li
1. [3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination](https://arxiv.org/abs/2406.05132), CVPR 2025, \
Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai
1. [Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2406.16449), ICML 2024, \
Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, Rongrong Ji
1. [Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models](https://nips.cc/virtual/2024/poster/93023), NeurIPS 2024, \
Hongliang Wei, Xingtao Wang, Xianqi Zhang, Xiaopeng Fan, Debin Zhao
1. [Multi-Object Hallucination in Vision Language Models](https://nips.cc/virtual/2024/poster/95666), NeurIPS 2024, \
Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu, Shengyi Qian, Jianing Yang, David Fouhey, Joyce Chai
1. [Mm-sap: A comprehensive benchmark for assessing self-awareness of multimodal large language models in perception](), arXiv 2024, \
Y. Wang, Y. Liao, H. Liu, H. Liu, Y. Wang, and Y. Wang
1. [Visual hallucinations of multi-modal large language models](), arXiv 2024, \
W. Huang, H. Liu, M. Guo, and N. Z. Gong
1. [How easy is it to fool your multimodal llms? an empirical analysis on deceptive prompts](), arXiv 2024, \
Y. Qian, H. Zhang, Y. Yang, and Z. Gan
1. [The all-seeing project v2: Towards general relation comprehension of the open world](), arXiv 2024, \
W. Wang, Y. Ren, H. Luo, T. Li, C. Yan, Z. Chen, W. Wang, Q. Li, L. Lu, X. Zhu et al.
1. [(POPE) Evaluating object hallucination in large vision-language models](https://arxiv.org/abs/2305.10355), EMNLP 2023, \
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen
1. [(NOPE) Negative object presence evaluation (nope) to measure object hallucination in vision-language models](), ALVR 2024, \
Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, et al.
1. [Genception: Evaluate multimodal llms with unlabeled unimodal data](), arXiv 2024, \
L. Cao, V. Buchner, Z. Senane, and F. Yang
1. [Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences](), arXiv 2024, \
X. Wang, Y. Zhou, X. Liu, H. Lu, Y. Xu, F. He, J. Yoon, T. Lu, G. Bertasius, M. Bansal et al.
1. [Unified hallucination detection for multimodal large language models](), arXiv 2024, \
X. Chen, C. Wang, Y. Xue, N. Zhang, X. Yang, Q. Li, Y. Shen, J. Gu, and H. Chen
1. [Ciem: Contrastive instruction evaluation method for better instruction tuning](), NeurIPS workshop 2023, \
Hongyu Hu, Jiyuan Zhang, Minyi Zhao, et al.
1. [(AMBER) An llm-free multi-dimensional benchmark for mllms hallucination evaluation](https://arxiv.org/abs/2311.07397), arXiv 2023, \
Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, Jitao Sang
1. [(EMMA) Evaluation and mitigation of agnosia in multimodal large language models](), arXiv 2023, \
J. Lu, J. Rao, K. Chen, X. Guo, Y. Zhang, B. Sun, C. Yang, and J. Yang
1. [Behind the magic, merlim: Multi-modal evaluation benchmark for large image-language models](), arXiv 2023, \
A. Villa, J. C. L. Alcazar, A. Soto, and B. Ghanem
1. [Mme: A comprehensive evaluation benchmark for multimodal large language models](https://arxiv.org/abs/2306.13394), arXiv 2023, \
Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji
1. [Hallusionbench: An advanced diagnostic suite for entangled language hallucination & visual illusion in large vision-language models](), arXiv 2023, \
T. Guan, F. Liu, X. Wu, R. Xian, Z. Li, X. Liu, X. Wang, L. Chen, F. Huang, Y. Yacoob et al
1. [(CHAIR) Object hallucination in image captioning](), EMNLP 2018, \
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, et al.