## Table of Contents
<table>
<tr><td colspan="2"><a href="#1-detection" style="color:#B22222">1. Detection</a></td></tr>
<tr>
  <td>&ensp;<a href="#11-automated-post-hoc-detection-models">1.1 Automated Post hoc Detection Models</a></td>
  <td>&ensp;<a href="#12-human-in-the-loop-detection">1.2 Human in the loop Detection</a></td>
</tr>
<tr>
  <td>&ensp;<a href="#13-task-specific-detection">1.3 Task Specific Detectors</a></td>
  <!-- <td>&ensp;<a href="#12-human-in-the-loop-detection">1.2 Human in the loop Detection</a></td> -->
</tr>

<tr><td colspan="2"><a href="#2-benchmarks" style="color:#B22222">2. Benchmarks</a></td></tr>

</table>

## [1. Detection](#content)
1. [HalLoc: Token-level Localization of Hallucinations for Vision Language Models](https://arxiv.org/abs/2506.10286), CVPR 2025, \
Eunkyu Park, Minyeong Kim, Gunhee Kim
1. [Hallo3D: Multi-Modal Hallucination Detection and Mitigation for Consistent 3D Content Generation](https://proceedings.neurips.cc/paper_files/paper/2024/hash/d75660d6eb0ce31360c768fef85301dd-Abstract-Conference.html), NeurIPS 2024, \
Hongbo Wang, Jie Cao, Jin Liu, Xiaoqiang Zhou, Huaibo Huang, Ran He
1. [Estimating the Hallucination Rate of Generative AI](https://nips.cc/virtual/2024/poster/95553), NeurIPS 2024, \
Andrew Jesson, Nicolas Beltran Velez, Quentin Chu, Sweta Karlekar, Jannik Kossen, Yarin Gal, John Cunningham, David Blei

### [1.1 Automated Post hoc Detection Models](#content)
1. [Visual hallucination detection in large vision-language models via evidential conflict](https://www.sciencedirect.com/science/article/abs/pii/S0888613X25001483), ICAR 2025, \
Tao Huang, Zhekun Liu, Rui Wang, Yang Zhang, Liping Jing

### [1.2 Human in the loop Detection](#content)
1. [LLaVA-RLHF: Aligning Large Multimodal Models with Factually Augmented RLHF](https://llava-rlhf.github.io/), arXiv 2023, \
Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, Trevor Darrell

### [1.3 Task Specific Detectors](#content)


## [2. Evaluation Methods](#content)
1. [Evaluation and mitigation of agnosia in multimodal large language models](), arXiv 2023, \
Jiaying Lu, Jinmeng Rao, Kezhen Chen, et al.
1. [Halle-switch: Rethinking and controlling object existence hallucinations in large vision language models for detailed caption](), arXiv 2023, \
Bohan Zhai, Shijia Yang, Xiangchen Zhao, et al.
1. [Faithscore: Evaluating hallucinations in large visionlanguage models](), arXiv 2023, \
Liqiang Jing, Ruosen Li, Yunmo Chen, et al.
1. [Mitigating hallucination in large multi-modal models via robust instruction tuning](), arXiv 2023, \
Fuxiao Liu, Kevin Lin, Linjie Li, et al.
1. [Aligning large multimodal models with factually augmented rlhf](), arXiv 2023, \
Zhiqing Sun, Sheng Shen, Shengcao Cao, et al.
1. [Detecting and preventing hallucinations in large vision language models](), arXiv 2023, \
Anisha Gunjal, Jihan Yin, and Erhan Bas
1. [Instructblip: Towards general-purpose visionlanguage models with instruction tuning](), arXiv 2023, \
Wenliang Dai, Junnan Li, Dongxu Li, et al.
1. [Evaluation and analysis of hallucination in large vision-language models](), arXiv 2023, \
Junyang Wang, Yiyang Zhou, Guohai Xu, et al.
1.[Object hallucination in image captioning](), EMNLP 2018, \
Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, et al.

## [3. Benchmark](#content)
1. [ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2409.09318), CVPR 2025, \
Yahan Tu, Rui Hu, Jitao Sang
1. [PhD: A ChatGPT-Prompted Visual Hallucination Evaluation Dataset](https://huggingface.co/datasets/AIMClab-RUC/PhD), CVPR 2025, \
Jiazhen Liu, Yuhan Fu, Ruobing Xie, Runquan Xie, Xingwu Sun, Fengzong Lian, Zhanhui Kang, Xirong Li
1. [3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination](https://arxiv.org/abs/2406.05132), CVPR 2025, \
Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai
1. [Evaluating and Analyzing Relationship Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2406.16449), ICML 2024, \
Mingrui Wu, Jiayi Ji, Oucheng Huang, Jiale Li, Yuhang Wu, Xiaoshuai Sun, Rongrong Ji
1. [Toward a Stable, Fair, and Comprehensive Evaluation of Object Hallucination in Large Vision-Language Models](https://nips.cc/virtual/2024/poster/93023), NeurIPS 2024, \
Hongliang Wei, Xingtao Wang, Xianqi Zhang, Xiaopeng Fan, Debin Zhao
1. [Multi-Object Hallucination in Vision Language Models](https://nips.cc/virtual/2024/poster/95666), NeurIPS 2024, \
Xuweiyi Chen, Ziqiao Ma, Xuejun Zhang, Sihan Xu, Shengyi Qian, Jianing Yang, David Fouhey, Joyce Chai
1. [(POPE) Evaluating object hallucination in large vision-language models](https://arxiv.org/abs/2305.10355), EMNLP 2023, \
Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen
1. [(NOPE) Negative object presence evaluation (nope) to measure object hallucination in vision-language models](), ALVR 2024, \
Holy Lovenia, Wenliang Dai, Samuel Cahyawijaya, et al.
1. [Ciem: Contrastive instruction evaluation method for better instruction tuning](), NeurIPS workshop 2023, \
Hongyu Hu, Jiyuan Zhang, Minyi Zhao, et al.
1. [(AMBER) An llm-free multi-dimensional benchmark for mllms hallucination evaluation](https://arxiv.org/abs/2311.07397), arXiv 2023, \
Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, Jitao Sang